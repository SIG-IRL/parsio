#[cfg(test)]
mod tests {
    use parsio_core::modules::tokenizer::Tokenizer;

    #[test]
    fn test_basic_tokenize() {
        let tokenizer = Tokenizer::new(false, false, false, false, false, false, false);
        let tokens = tokenizer.tokenize("Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§");
        assert_eq!(tokens, vec!["Ø³Ù„Ø§Ù…", "Ø¬Ù‡Ø§Ù†"]);
    }

    #[test]
    fn test_zwnj_split() {
        let tokenizer = Tokenizer::new(false, true, false, false, false, false, false);
        let tokens = tokenizer.tokenize("Ù…ÛŒ\u{200c}Ø±ÙˆÙ…");
        assert_eq!(tokens, vec!["Ù…ÛŒ", "Ø±ÙˆÙ…"]);
    }

    #[test]
    fn test_verb_joining() {
        let tokenizer = Tokenizer::new(true, false, false, false, false, false, false);
        let tokens = tokenizer.tokenize("Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª");
        assert_eq!(tokens, vec!["Ú¯ÙØªÙ‡_Ø´Ø¯Ù‡_Ø§Ø³Øª"]);
    }

    #[test]
    fn test_mixed_language() {
        let tokenizer = Tokenizer::new(false, false, false, false, false, false, false);
        let tokens = tokenizer.tokenize("helloØ³Ù„Ø§Ù…");
        assert_eq!(tokens, vec!["hello", "Ø³Ù„Ø§Ù…"]);
    }

    #[test]
    fn test_emoji_separation() {
        let tokenizer = Tokenizer::new(false, false, true, false, false, false, false);
        let tokens = tokenizer.tokenize("Ø®ÙˆØ¨Ù…ğŸ˜‚");
        assert_eq!(tokens, vec!["Ø®ÙˆØ¨Ù…", "ğŸ˜‚"]);
    }
}
